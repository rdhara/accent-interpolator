{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from preprocess import generate_data_loaders\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader_dr1, test_loader_dr1 = generate_data_loaders('DR1', batch_size=4)\n",
    "train_loader_dr5, test_loader_dr5 = generate_data_loaders('DR5', batch_size=4)\n",
    "\n",
    "input_dim = 128\n",
    "hidden_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, 400)\n",
    "        self.fc4 = nn.Linear(400, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x, encoder):\n",
    "        mu, logvar = encoder(x.view(-1, self.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=20):\n",
    "        super(SharedEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.fc21 = nn.Linear(400, hidden_dim)\n",
    "        self.fc22 = nn.Linear(400, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = VAE(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "model_2 = VAE(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "encoder = SharedEncoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=chain(*[\n",
    "        model_1.parameters(),\n",
    "        model_2.parameters(),\n",
    "        encoder.parameters()\n",
    "    ]),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, 128), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "\n",
    "def train(epoch, train_loader_1, train_loader_2):\n",
    "    \n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    encoder.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader_1):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model_1(data, encoder)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    for batch_idx, data in enumerate(train_loader_2):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model_2(data, encoder)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader_1.dataset) + len(train_loader_2.dataset)\n",
    "    print('====> Epoch: {}\\n\\tTraining set loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(epoch, test_loader_1, test_loader_2):\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    encoder.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader_1):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model_1(data, encoder)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            \n",
    "        for i, data in enumerate(test_loader_2):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model_2(data, encoder)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "\n",
    "    test_loss /=  len(test_loader_1.dataset) + len(test_loader_2.dataset)\n",
    "    print('\\tTest set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "train_trajectory, test_trajectory = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    tr_loss = train(epoch, train_loader_dr1, train_loader_dr5)\n",
    "    train_trajectory.append(tr_loss)\n",
    "    vl_loss =  test(epoch, train_loader_dr1, train_loader_dr5)\n",
    "    test_trajectory.append(vl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (REC + KL)')\n",
    "plt.plot(train_trajectory, c='b', label='Training')\n",
    "plt.plot(test_trajectory, c='r', label='Validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
